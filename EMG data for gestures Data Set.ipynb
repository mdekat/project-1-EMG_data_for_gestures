{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMG data for gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0. Importing libraries\n",
    "\n",
    "# Stop warnings from scikit-learn\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "#==== Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. defining functions for main programm - part 1\n",
    "\n",
    "def getdata(): #========= importing data from different files\n",
    "    \n",
    "    filename = 'E:\\\\ML\\\\ML test 1 - UCI database - EMG data for gestures Data Set\\\\train_data_set2.csv'\n",
    "    dataset = pd.read_csv(filename, sep = ';')\n",
    "    X = dataset.iloc[:,:-1].values\n",
    "    y = dataset.iloc[:,-1].values\n",
    "    y.reshape((y.shape[0],1))\n",
    "    \n",
    "    print('Data loaded...')\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def prepdata(X,y): #==== preparing data for Model\n",
    "    \n",
    "    #--- Splitting the dataset into the Training set and Test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 123)\n",
    "    y.reshape((y.shape[0],1))\n",
    "\n",
    "    #--- Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    print('Data prepared...')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def makeandrunmodel(X_train, X_test, y_train, y_test): #==== Preparing and fitting Model\n",
    "\n",
    "    #--- Logistic Regression\n",
    "    \n",
    "    classifier = RandomForestClassifier(n_estimators = 4, criterion = 'entropy', random_state = 123)\n",
    "    classifier.fit(X_train,y_train)\n",
    "\n",
    "    #--- Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print('Model compiled and fitted...')\n",
    "    \n",
    "    return y_pred, classifier\n",
    "\n",
    "def showresults(y_test, y_pred, y): #=== presenting results\n",
    "    \n",
    "    print('\\nResults: ')\n",
    "    #--- Making the Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    #--- Calculate ratio\n",
    "    ttp = 0\n",
    "    for i in range(int(y.max())+1):\n",
    "        tp = cm[i,i]\n",
    "        n = np.sum(cm[:,i])\n",
    "        acc = tp/n\n",
    "        ttp += cm[i,i]\n",
    "        print('{}) {} There are {} predictions of {} which is {:0.2f}% of the total amount.'.format(i+1,cm[i], tp, i, acc*100))\n",
    "    print('Total accuracy of {:0.2f}%\\n'.format(ttp/sum(sum(cm))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded...\n",
      "Data prepared...\n",
      "Model compiled and fitted...\n",
      "\n",
      "Results: \n",
      "1) [620   6   5   0   0   2] There are 620 predictions of 0 which is 85.28% of the total amount.\n",
      "2) [ 12 542  85   9   4  43] There are 542 predictions of 1 which is 62.08% of the total amount.\n",
      "3) [ 34 131 963  28   6  70] There are 963 predictions of 2 which is 77.16% of the total amount.\n",
      "4) [ 24  55  58 798  68  56] There are 798 predictions of 3 which is 73.08% of the total amount.\n",
      "5) [ 12  13  24 169 237  11] There are 237 predictions of 4 which is 71.17% of the total amount.\n",
      "6) [ 25 126 113  88  18 438] There are 438 predictions of 5 which is 70.65% of the total amount.\n",
      "Total accuracy of 73.53%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MAIN PROGRAMM - part 1\n",
    "\n",
    "X,y = getdata() # load data\n",
    "X_train, X_test, y_train, y_test = prepdata(X,y) # prep data\n",
    "y_pred, classifier = makeandrunmodel(X_train, X_test, y_train, y_test) # compile and run model\n",
    "showresults(y_test, y_pred, y) # show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for additional programm: randomsearch for the best parameters\n",
    "\n",
    "def makegrid():\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [2, 8, 32, 64, 256, 512, 1024]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10, 20]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1,2,4,8]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "    \n",
    "    return random_grid\n",
    "\n",
    "def randomsearchfit(classifier, random_grid, X_train, y_train):\n",
    "    \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    classifier_random = RandomizedSearchCV(estimator = classifier, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    classifier_random.fit(X_train, y_train)\n",
    "    \n",
    "    print(classifier_random.best_params_)\n",
    "    \n",
    "    return classifier_random.best_estimator_\n",
    "\n",
    "def runbasemodel(X_train, y_train, X_test, y_test):\n",
    "    base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "    base_model.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print('Results of basemodel')\n",
    "    showresults(y_test, y_pred, y)\n",
    "\n",
    "def runbestmodel(best_random, X_train, y_train, X_test, y_test):\n",
    "    y_pred = best_random.predict(X_test)\n",
    "    print('Results of best model')\n",
    "    showresults(y_test, y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 20.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 512, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 70, 'bootstrap': False}\n",
      "Results of basemodel\n",
      "\n",
      "Results: \n",
      "1) [620   6   5   0   0   2] There are 620 predictions of 0 which is 85.28% of the total amount.\n",
      "2) [ 12 542  85   9   4  43] There are 542 predictions of 1 which is 62.08% of the total amount.\n",
      "3) [ 34 131 963  28   6  70] There are 963 predictions of 2 which is 77.16% of the total amount.\n",
      "4) [ 24  55  58 798  68  56] There are 798 predictions of 3 which is 73.08% of the total amount.\n",
      "5) [ 12  13  24 169 237  11] There are 237 predictions of 4 which is 71.17% of the total amount.\n",
      "6) [ 25 126 113  88  18 438] There are 438 predictions of 5 which is 70.65% of the total amount.\n",
      "Total accuracy of 73.53%\n",
      "\n",
      "Results of best model\n",
      "\n",
      "Results: \n",
      "1) [624   3   6   0   0   0] There are 624 predictions of 0 which is 98.11% of the total amount.\n",
      "2) [  3 630  43   1   1  17] There are 630 predictions of 1 which is 89.87% of the total amount.\n",
      "3) [   7   29 1178    4    0   14] There are 1178 predictions of 2 which is 93.20% of the total amount.\n",
      "4) [   0    0    5 1004   29   21] There are 1004 predictions of 3 which is 87.15% of the total amount.\n",
      "5) [  0   2   6 121 337   0] There are 337 predictions of 4 which is 91.83% of the total amount.\n",
      "6) [  2  37  26  22   0 721] There are 721 predictions of 5 which is 93.27% of the total amount.\n",
      "Total accuracy of 91.85%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ADDITIONAL PROGRAMM - randomsearch for the best parameters\n",
    "\n",
    "random_grid = makegrid()  # define the grid to be searched\n",
    "best_random = randomsearchfit(classifier, random_grid, X_train, y_train)  # randomsearchfit with best parameters as output\n",
    "runbasemodel(X_train, y_train, X_test, y_test)  # accuracy of a basemodel for comparison\n",
    "runbestmodel(best_random, X_train, y_train, X_test, y_test)  # accuracy of a model with the best parameters according to the search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next step can be a gridsearch with the module 'GridSearchCV' to narrow down the best parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
